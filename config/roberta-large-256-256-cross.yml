title: roberta-large-256-256-cross-train
log_dir: ../log/
mode: train
use_wandb: False
seeds: [1, 2, 3, 4, 5]
data:
  path: ../data/origin
  num_workers: 12
  num_classes: 4
  query_seq_len: 256
  candidate_seq_len: 256
  augmentation: False
model:
  type: cross
  encoder: ../models/chinese-roberta-wwm-ext-large
  dropout: 0.1
  device: "cuda:2"
  device_ids: [2]
  loss_fn: BCEWithLogitsLoss
  pooler: cls
  head: regression
optimizer:
  accumulation_steps: 1
  name: Adam
  lr: 0.00002
  scheduler: cos
train:
  batch_size: 16
  eval_batch_size: 100
  num_epochs: 2
