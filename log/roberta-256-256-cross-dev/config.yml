data:
    augmentation: false
    candidate_seq_len: 256
    num_classes: 4
    num_workers: 12
    path: ../data/bm25
    query_seq_len: 256
log_dir: ../log/
mode: train_dev
model:
    device: cuda:2
    device_ids:
    - 2
    dropout: 0.1
    encoder: ../models/chinese-roberta-wwm-ext
    head: regression
    loss_fn: BCEWithLogitsLoss
    pooler: cls
    type: cross
optimizer:
    accumulation_steps: 1
    lr: 2.0e-05
    name: Adam
    scheduler: cos
seeds:
- 1
title: roberta-256-256-cross-dev
train:
    batch_size: 32
    eval_batch_size: 100
    num_epochs: 10
use_wandb: false
