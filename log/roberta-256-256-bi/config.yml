data:
    augmentation: false
    candidate_seq_len: 256
    num_classes: 4
    num_workers: 12
    path: ../data/origin
    query_seq_len: 256
log_dir: ../log/
mode: train_dev
model:
    device: cuda:3
    device_ids:
    - 3
    dropout: 0.1
    encoder: ../models/chinese-roberta-wwm-ext
    head: cosine
    loss_fn: MSELoss
    pooler: mean
    type: bi
optimizer:
    accumulation_steps: 1
    lr: 2.0e-05
    name: Adam
    scheduler: cos
seeds:
- 1
- 2
- 3
title: roberta-256-256-bi
train:
    batch_size: 32
    eval_batch_size: 200
    num_epochs: 10
use_wandb: false
