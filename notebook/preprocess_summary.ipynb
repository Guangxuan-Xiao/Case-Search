{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/chinese-roberta-wwm-ext\")\n",
    "crime_pattern = re.compile(r'已构成(.*?)罪')\n",
    "from summary import unique_sentences, split_sentence, score_sentences, get_text_after_last_startword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_list(l):\n",
    "    return {\n",
    "        'mean': np.mean(l),\n",
    "        'std': np.std(l),\n",
    "        'max': np.max(l),\n",
    "        'min': np.min(l),\n",
    "        'len': len(l),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/197 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.578 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 197/197 [07:33<00:00,  2.30s/it]\n",
      "ic| len(train_inputs): 19915\n",
      "    len(train_edges): 19718\n",
      "    len(train_query_ridxs): 197\n",
      "    len(train_node_graph_ids): 19915\n",
      "    len(train_edge_graph_ids): 19718\n",
      "    len(train_candidate_ridxs): 19718\n",
      "    len(train_labels): 19718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19915, 19718, 197, 19915, 19718, 19718, 19718)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sent_len = []\n",
    "candidate_sent_len = []\n",
    "def preprocess_data(data_path, has_label=False):\n",
    "    query_path = osp.join(data_path, 'query.json')\n",
    "    all_candidates_path = osp.join(data_path, 'candidates')\n",
    "    if has_label:\n",
    "        label_path = osp.join(data_path, 'label_top30_dict.json')\n",
    "        label = json.load(open(label_path))\n",
    "        labels = []\n",
    "    edges, inputs, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs = [\n",
    "    ], [], [], [], [], []\n",
    "    with open(query_path) as f:\n",
    "        query_lines = f.readlines()\n",
    "    node_idx = 0\n",
    "    for graph_idx, query_line in enumerate(tqdm(query_lines[:])):\n",
    "        sentences_zoo = []\n",
    "        sentences_ptr = [0]\n",
    "        query_line = query_line.strip()\n",
    "        query_dict = json.loads(query_line)\n",
    "        query_text = \"。\".join(query_dict['crime']) + \\\n",
    "            \"。\"+get_text_after_last_startword(query_dict['q'])\n",
    "        query_sentences = list(split_sentence(query_text))\n",
    "        unique_query_sentences = unique_sentences(query_sentences)\n",
    "        sentences_zoo += unique_query_sentences\n",
    "        sentences_ptr.append(len(sentences_zoo))\n",
    "        query_sent_len.append(len(unique_query_sentences))\n",
    "        node_graph_ids.append(graph_idx)\n",
    "        query_ridxs.append(query_dict['ridx'])\n",
    "        query_ridx = str(query_dict['ridx'])\n",
    "        query_idx = node_idx\n",
    "        node_idx += 1\n",
    "        candidates_path = osp.join(all_candidates_path, query_ridx)\n",
    "        for candidate in os.listdir(candidates_path):\n",
    "            candidate_ridx = candidate[:-5]\n",
    "            candidate_path = osp.join(candidates_path, candidate)\n",
    "            candidate_dict = json.load(open(candidate_path))\n",
    "            all_text = ''.join(candidate_dict.values())\n",
    "            crime_name = crime_pattern.search(all_text)\n",
    "            if crime_name is None:\n",
    "                crime_name = ''\n",
    "            else:\n",
    "                crime_name = crime_name.group(1) + '罪'\n",
    "            candidate_text = '。'.join(\n",
    "                [crime_name, get_text_after_last_startword(candidate_dict['ajjbqk'])])\n",
    "            candidate_sentences = list(split_sentence(candidate_text))\n",
    "            unique_candidate_sentences = unique_sentences(\n",
    "                candidate_sentences)\n",
    "            sentences_zoo += unique_candidate_sentences\n",
    "            sentences_ptr.append(len(sentences_zoo))\n",
    "            candidate_sent_len.append(len(unique_candidate_sentences))\n",
    "            candidate_idx = node_idx\n",
    "            node_idx += 1\n",
    "            node_graph_ids.append(graph_idx)\n",
    "            edge_graph_ids.append(graph_idx)\n",
    "            edges.append([query_idx, candidate_idx])\n",
    "            candidate_ridxs.append(int(candidate_ridx))\n",
    "            if has_label:\n",
    "                if candidate_ridx in label[query_ridx]:\n",
    "                    labels.append(label[query_ridx][candidate_ridx])\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "        sentence_scores = score_sentences(sentences_zoo)\n",
    "        for l, r in zip(sentences_ptr[:-1], sentences_ptr[1:]):\n",
    "            sentences = sentences_zoo[l:r]\n",
    "            sentences_score = sentence_scores[l:r]\n",
    "            sentences = [sentence for _, sentence in sorted(\n",
    "                zip(sentences_score, sentences), key=lambda x: x[0], reverse=True)]\n",
    "            text = ''.join(sentences)\n",
    "            tokenized_text = tokenizer(\n",
    "                text, return_tensors=\"pt\")\n",
    "            inputs.append(tokenized_text)\n",
    "    if has_label:\n",
    "        return inputs, edges, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs, labels\n",
    "    return inputs, edges, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs\n",
    "\n",
    "\n",
    "train_path = '../data/summary/train'\n",
    "train_inputs, train_edges, train_query_ridxs, train_node_graph_ids, train_edge_graph_ids, train_candidate_ridxs, train_labels = preprocess_data(\n",
    "    train_path, has_label=True)\n",
    "ic(len(train_inputs), len(train_edges), len(train_query_ridxs), len(train_node_graph_ids),\n",
    "   len(train_edge_graph_ids), len(train_candidate_ridxs), len(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| stat_list(query_sent_len): {'len': 10, 'max': 14, 'mean': 6.9, 'min': 4, 'std': 3.014962686336267}\n",
      "    stat_list(candidate_sent_len): {'len': 1003,\n",
      "                                    'max': 768,\n",
      "                                    'mean': 84.7308075772682,\n",
      "                                    'min': 2,\n",
      "                                    'std': 105.39177879196771}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mean': 6.9, 'std': 3.014962686336267, 'max': 14, 'min': 4, 'len': 10},\n",
       " {'mean': 84.7308075772682,\n",
       "  'std': 105.39177879196771,\n",
       "  'max': 768,\n",
       "  'min': 2,\n",
       "  'len': 1003})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(stat_list(query_sent_len), stat_list(candidate_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(processed_path, edges, inputs, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs, labels=None):\n",
    "    torch.save(inputs, osp.join(processed_path, 'inputs.pt'))\n",
    "    torch.save(edges, osp.join(processed_path, 'edges.pt'))\n",
    "    torch.save(query_ridxs, osp.join(processed_path, 'query_ridxs.pt'))\n",
    "    torch.save(candidate_ridxs, osp.join(processed_path, 'candidate_ridxs.pt'))\n",
    "    torch.save(node_graph_ids, osp.join(processed_path, 'node_graph_ids.pt'))\n",
    "    torch.save(edge_graph_ids, osp.join(processed_path, 'edge_graph_ids.pt'))\n",
    "    if labels is not None:\n",
    "        torch.save(labels, osp.join(processed_path, 'labels.pt'))\n",
    "train_processed_path = '../data/summary/train/processed'\n",
    "save(train_processed_path, train_edges, train_inputs, train_query_ridxs,\n",
    "     train_node_graph_ids, train_edge_graph_ids, train_candidate_ridxs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:30<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = '../data/summary/test'\n",
    "test_inputs, test_edges, test_query_ridxs, test_node_graph_ids, test_edge_graph_ids, test_candidate_ridxs = preprocess_data(\n",
    "    test_path, has_label=False)\n",
    "test_processed_path = '../data/summary/test/processed'\n",
    "save(test_processed_path, test_edges, test_inputs, test_query_ridxs,\n",
    "     test_node_graph_ids, test_edge_graph_ids, test_candidate_ridxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_query_ridxs = train_query_ridxs[:17]\n",
    "edge_split_idx = (np.array(train_edge_graph_ids) < 17).sum()\n",
    "node_split_idx = (np.array(train_node_graph_ids) < 17).sum()\n",
    "val_inputs = train_inputs[:node_split_idx]\n",
    "val_edges = train_edges[:edge_split_idx]\n",
    "val_labels = train_labels[:edge_split_idx]\n",
    "val_candidate_ridxs = train_candidate_ridxs[:edge_split_idx]\n",
    "val_node_graph_ids = train_node_graph_ids[:node_split_idx]\n",
    "val_edge_graph_ids = train_edge_graph_ids[:edge_split_idx]\n",
    "val_processed_path = '../data/summary/val/processed'\n",
    "os.makedirs(val_processed_path, exist_ok=True)\n",
    "save(val_processed_path, val_edges, val_inputs, val_query_ridxs,\n",
    "     val_node_graph_ids, val_edge_graph_ids, val_candidate_ridxs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_query_ridxs = train_query_ridxs[17:]\n",
    "train_dev_inputs = train_inputs[node_split_idx:]\n",
    "train_dev_edges = np.array(train_edges[edge_split_idx:]) - node_split_idx\n",
    "train_dev_edges = train_dev_edges.tolist()\n",
    "train_dev_labels = train_labels[edge_split_idx:]\n",
    "train_dev_candidate_ridxs = train_candidate_ridxs[edge_split_idx:]\n",
    "train_dev_node_graph_ids = np.array(train_node_graph_ids[node_split_idx:]) - 17\n",
    "train_dev_edge_graph_ids = np.array(train_edge_graph_ids[edge_split_idx:]) - 17\n",
    "train_dev_node_graph_idx = train_dev_node_graph_ids.tolist()\n",
    "train_dev_edge_graph_idx = train_dev_edge_graph_ids.tolist()\n",
    "train_dev_processed_path = '../data/summary/train_dev/processed'\n",
    "os.makedirs(train_dev_processed_path, exist_ok=True)\n",
    "save(train_dev_processed_path, train_dev_edges, train_dev_inputs,\n",
    "     train_dev_query_ridxs, train_dev_node_graph_ids, train_dev_edge_graph_ids,\n",
    "     train_dev_candidate_ridxs, train_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3db371a07bed52793c8840e411d9d35d61e1cfd36a2896481af3a875f3ddc4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('search': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
