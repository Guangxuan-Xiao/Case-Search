{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from gensim.summarization import bm25\n",
    "import jieba\n",
    "import numpy as np\n",
    "from summary import stopwords, split_sentence, unique_sentences\n",
    "from icecream import ic\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/chinese-roberta-wwm-ext\")\n",
    "crime_pattern = re.compile(r'已构成(.*?)罪')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = \"2018年1月15日14时10分许，被告人莫新国酒后驾驶湘A×××××号小型轿车沿长沙市天心区伊莱克斯大道由南往北行驶至水电八局基地路段时被在该处执勤的长沙市公安局交通警察支队民警检查，经现场酒精吹气检测，测试结果显示其血液中乙醇含量为195毫克／100毫升，随即被告人莫新国被交警带至湖南省融城医院抽取血样，并将血样送至长沙市公安局物证鉴定所检验，经检验，其血液中乙醇含量为201.1毫克／100毫升。2009年11月15日，被告人莫新国经长沙市残疾人联合会审核为精神残疾人。2018年5月28日，经湖南省芙蓉司法鉴定中心鉴定，被告人莫新国作案时处于普通醉酒状态，实施危害行为时有完全刑事责任能力。2018年1月30日，被告人莫新国主动到公安机关投案，其归案后如实供述了自己的罪行。\"\n",
    "\n",
    "example_candidate = \" 福州市鼓楼区人民检察院指控，1、2017年3月7日21时30分左右，被告人林隆川在福州市晋安区鹤林新城小区对面公交车站以人民币300元的价格将一袋净重0.64克的甲基苯丙胺（冰毒）卖给胡某4。 2、同日22时左右，胡某，4再次在上述地点以人民币300元的价格向被告人林隆川购买了一袋净重0.62克的甲基苯丙胺。 3、同日23时30分左右，陈某，4与被告人林隆川联系以人民币300元的价格购买一袋净重0.68克的甲基苯丙胺，被告人林隆川在福州市晋安区鹤林新城某座楼下交付毒品时，被公安民警当场抓获。 被告人林隆川三次贩卖毒品净重共1.94克。经鉴定：上述缴获的毒品均含有甲基苯丙胺成份。 对上述指控，公诉机关当庭宣读和出示了相关证据。通过举证，公诉机关认为被告人林隆川的行为已触犯了《中华人民共和国刑法》第三百四十七条第一款、第四款之规定，应以贩卖毒品罪追究其刑事责任，因被告人林隆川系累犯，应当从重处罚，建议对其在有期徒刑三年六个月至四年六个月的幅度内处刑，并处罚金，提请法院依法予以惩处。 被告人林隆川在庭审过程中对起诉书指控的第1、2起犯罪事实没有异议，但认为起诉书指控的第3起不是事实，辩称之前其曾答应陈某，4贩卖一个毒品给他，但是后来改变主意不想卖给陈某，4了，其当时下楼是为了买宵夜。 经审理查明，1、2017年3月7日21时30分左右，被告人林隆川在福州市晋安区鹤林新城小区对面公交车站以人民币300元的价格将一袋净重0.64克的甲基苯丙胺（冰毒）卖给胡某，4。 2、同日22时左右，胡某，4再次在上述地点以人民币300元的价格向被告人林隆川购买了一袋净重0.62克的甲基苯丙胺。 3、同日23时30分左右，陈某，4与被告人林隆川联系以人民币300元的价格购买一袋净重0.68克的甲基苯丙胺，被告人林隆川在福州市晋安区鹤林新城某座楼下交付毒品时，被公安民警当场抓获。 上述事实有下列经庭审举证、质证的证据证实： 1、证人陈某，4的证言及相关辨认笔录证实：2017年3月7日下午14时许，其通过微信联系胡某，4问他是否有冰毒可以拿，胡某，4答复可以带其去找一个朋友以一个冰毒人民币300元的价格买，其便到派出所举报了这件事。在民警的安排下，当天下午18时许，其在福州市鼓楼区洪山桥头附近中石化加油站接到胡某，4以及胡某，4带来的小弟杨某，4，当时其负责开车，胡某，4坐在副驾驶位，杨某，4坐在后排，三人一起到福州市晋安区鹤林新城对面的公交站等贩卖毒品的人出现。途中胡某，4表示要先将人民币300元通过微信转给贩卖毒品的那人，其便给胡某，4现金人民币300元，胡某，4通过微信转了人民币300元给卖毒品的人。2017年3月7日晚上21时许，其在鹤林新城对面公交站看见贩卖毒品的人驾驶着一部黑色现代牌小车出现，那人将车开到其车旁，放下玻璃，扔出一个蓝色七匹狼烟盒便开车走了。其下车捡起烟盒并交给胡某，4，胡某，4打开烟盒从里面拿出一个用透明自封袋包装的白色晶体交给其，之后其开车带着胡某，4、杨某，4离开鹤林新城，途中，其给胡某，4、杨某，4各200元人民币辛苦费，杨某，4没有收，胡某，4收下后说一个冰毒太少不够玩，主动联系刚才贩卖毒品的人通过微信转账人民币300元再买一个冰毒，还让其把车开回鹤林新城对面的公交站。2017年3月7日晚上22时左右，其开车带着胡某，4、杨某，4回到鹤林新城对面的公交站，发现刚才贩卖毒品的人已经开着车停在那里等，其就将车靠过去，那人和上次一样把玻璃放下扔出一个纸团后将车开走。那人在放下玻璃时其突然认出是被告人林隆川，微信名称是“政府高级官员”。胡某，4将纸团内的一袋透明自封袋包装白色晶体放进自己上衣口袋内后，其开车带着胡某，4、杨某，4离开，随后民警将胡某，4、杨某，4抓获。后来民警得知其认识刚才贩卖毒品的被告人林隆川，便让其将被告人林隆川引出来，其立即与被告人林隆川微信联系并转账给他人民币300元购买冰毒，被告人林隆川让其到鹤林新城某栋楼下等他，其和民警到指定地点后没多久被告人林隆川就下楼了，民警迅速将被告人林隆川抓获并从他身上查获一袋透明自封袋包装的白色晶体。 2、证人胡某，4的证言及相关辨认笔录证实：2017年3月7日下午16时左右，陈某，4微信询问其是否有地方可以买冰毒，其让陈某，4和其一起找被告人林隆川拿冰毒，其和被告人林隆川商定以人民币300元的价格购买一个冰毒，并约定好在福州市晋安区鹤林新城对面的公交站交易。当晚18时许，陈某，4开车在洪山桥头附近接上其和其朋友杨某，4，三人一起坐车到福州市晋安区鹤林新城对面公交站旁边等被告人林隆川，途中陈某，4将现金人民币300元给其，其通过微信转账人民币300元给被告人林隆川。当晚21时许，被告人林隆川开车来到其车边上，将窗户放下扔出一个蓝色七匹狼烟盒，然后就开车走了。陈某，4下车捡起烟盒交给其，其从烟盒拿出一袋塑料自封袋包装的白色晶体，确认是冰毒后交给陈某4，陈某，4拿到冰毒后开车带着其和杨某，4离开公交站，在车上陈某，4觉得一个冰毒不够玩让其也买一个，其找杨某，4凑足人民币300元通过微信转账给被告人林隆川，同时和被告人林隆川约定交易地点还是在福州市晋安区鹤林新城对面的公交站。2017年3月7日22时左右，陈某，4驾车载着其和杨某，4来到约定地点，被告人林隆川现代牌的车已经在公交站等待，被告人林隆川见其车靠近，将车窗放下扔出一团纸巾后就离开了。杨某，4下车将纸团捡起交给其，其打开纸巾发现里面是一袋塑料自封袋包装的白色晶体便放进自己上衣口袋，陈某，4开车带其离开，之后警察将其抓获。 3、证人杨某，4的证言及辨认笔录证实，2017年3月7日18时许，胡某，4告诉其他的朋友要买冰毒，让其一起去。之后，胡某，4的朋友开车载着其和胡某，4到福州市晋安区鹤林新城公交站旁和贩卖毒品的人交易。在车上胡某，4一直和贩卖毒品的人联系，并转账人民币300元给卖毒人，当晚21时许，一辆黑色小车停在其车旁边，开车男子打开车窗后扔出一个蓝色烟盒后开车离开。胡某，4的朋友捡起烟盒拿给胡某，4，胡某，4拿出烟盒里的白色塑料自封袋又交给他朋友，三人就驾车离开。途中，开车的男子对胡某，4说一个冰毒怕不够玩，能否再买一个。胡某，4又联系卖毒人转账人民币300元再买个毒品，交易地点还是约定在鹤林新城公交站。当其车到达交易地点时，那个卖毒品的人已经开车在那里等待，他朝其车子挡风玻璃扔了一包冰毒后开车离开。其将冰毒捡起来交给胡某，4，胡某，4朋友开车带着其和胡某，4离开，之后其、胡某，4就被警察抓获。 4、被告人林隆川在公安机关的原供述称：2017年3月7日晚上7点左右，胡某，4联系其购买冰毒并通过微信转账给其人民币300元，双方约定在鹤林新城对面的公交车站交易，其到福州市晋安区麻将馆内找到“黄某”，以人民币1200元的价格购买了四个冰毒。拿完冰毒其回到福州市晋安区鹤林新城某栋的家中，胡某，4微信催其交易，其驾驶闽A×××××号现代小车到鹤林新城对面公交车站，看到胡某，4的车停在那里，其将一袋冰毒扔向胡某，4的车子后离开。大约是当晚10点，其回到家中后胡某，4又联系其再买一袋冰毒，并通过微信转账给其人民币300元，约定交易地点不变。其再次驾驶闽A×××××号现代小车到了之前交易的公交站附近，几分钟后胡某，4的车停到其车边上，其将一袋冰毒往对方小车挡风玻璃上一扔便开车离开。其回到家中玩了一个多小时，陈某，4转账人民币300元给其向其购买冰毒，其让他到鹤林新城某号楼楼下拿冰毒，其拿了一袋冰毒到一楼准备卖给陈某，4时被公安民警抓获。 5、扣押的涉案毒品照片等物证、福州市公安局鼓楼分局出具的扣押清单、福建省毒品实物缴交收据等书证及福建正中司法鉴定所出具的毒品定性司法鉴定检验报告书，证实被告人林隆川贩卖的冰毒中检出甲基苯丙胺成分，数量为1.94克，现毒品已上缴。 6、微信聊天记录、微信支付记录等电子数据及相关辨认笔录，证实被告人林隆川与证人胡某，4、陈某，4协商交易冰毒、收取毒资的情况。 7、到案经过，证实被告人林隆川系由公安机关抓获归案。 8、户籍证明、前科材料等其他证据材料。 上述证据本院予以确认。 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.624 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2017年3月7日晚上22时左右，其开车带着胡某，4、其再次驾驶闽A×××××号现代小车到了之前交易的公交站附近，几分钟后胡某，4的车停到其车边上，其将一袋冰毒往对方小车挡风玻璃上一扔便开车离开。拿完冰毒其回到福州市晋安区鹤林新城某栋的家中，胡某，4微信催其交易，其驾驶闽A×××××号现代小车到鹤林新城对面公交车站，看到胡某，4的车停在那里，其将一袋冰毒扔向胡某，4的车子后离开。2017年3月7日21时30分左右，被告人林隆川在福州市晋安区鹤林新城小区对面公交车站以人民币300元的价格将一袋净重0.64克的甲基苯丙胺（冰毒）卖给胡某，4。2017年3月7日22时左右，陈某，4驾车载着其和杨某，4来到约定地点，被告人林隆川现代牌的车已经在公交站等待，被告人林隆川见其车靠近，将车窗放下扔出一团纸巾后就离开了。证人杨某，4的证言及辨认笔录证实，2017年3月7日18时许，胡某，4告诉其他的朋友要买冰毒，让其一起去。2017年3月7日下午16时左右，陈某，4微信询问其是否有地方可以买冰毒，其让陈某，4和其一起找被告人林隆川拿冰毒，其和被告人林隆川商定以人民币300元的价格购买一个冰毒，并约定好在福州市晋安区鹤林新城对面的公交站交易。2017年3月7日下午14时许，其通过微信联系胡某，4问他是否有冰毒可以拿，胡某，4答复可以带其去找一个朋友以一个冰毒人民币300元的价格买，其便到派出所举报了这件事。福建省毒品实物缴交收据等书证及福建正中司法鉴定所出具的毒品定性司法鉴定检验报告书，证实被告人林隆川贩卖的冰毒中检出甲基苯丙胺成分，数量为1.94克，现毒品已上缴。被告人林隆川在公安机关的原供述称：2017年3月7日晚上21时许，其在鹤林新城对面公交站看见贩卖毒品的人驾驶着一部黑色现代牌小车出现，那人将车开到其车旁，放下玻璃，扔出一个蓝色七匹狼烟盒便开车走了。被告人林隆川在庭审过程中对起诉书指控的第1、2017年3月7日晚上7点左右，胡某，4联系其购买冰毒并通过微信转账给其人民币300元，双方约定在鹤林新城对面的公交车站交易，其到福州市晋安区麻将馆内找到“黄某”，以人民币1200元的价格购买了四个冰毒。经鉴定：同日23时30分左右，陈某，4与被告人林隆川联系以人民币300元的价格购买一袋净重0.68克的甲基苯丙胺，被告人林隆川在福州市晋安区鹤林新城某座楼下交付毒品时，被公安民警当场抓获。到案经过，证实被告人林隆川系由公安机关抓获归案。后来民警得知其认识刚才贩卖毒品的被告人林隆川，便让其将被告人林隆川引出来，其立即与被告人林隆川微信联系并转账给他人民币300元购买冰毒，被告人林隆川让其到鹤林新城某栋楼下等他，其和民警到指定地点后没多久被告人林隆川就下楼了，民警迅速将被告人林隆川抓获并从他身上查获一袋透明自封袋包装的白色晶体。福州市公安局鼓楼分局出具的扣押清单、同日22时左右，胡某，4再次在上述地点以人民币300元的价格向被告人林隆川购买了一袋净重0.62克的甲基苯丙胺。通过举证，公诉机关认为被告人林隆川的行为已触犯了《中华人民共和国刑法》第三百四十七条第一款、第四款之规定，应以贩卖毒品罪追究其刑事责任，因被告人林隆川系累犯，应当从重处罚，建议对其在有期徒刑三年六个月至四年六个月的幅度内处刑，并处罚金，提请法院依法予以惩处。被告人林隆川三次贩卖毒品净重共1.94克。扣押的涉案毒品照片等物证、那人在放下玻璃时其突然认出是被告人林隆川，微信名称是“政府高级官员”。微信支付记录等电子数据及相关辨认笔录，证实被告人林隆川与证人胡某，4、当其车到达交易地点时，那个卖毒品的人已经开车在那里等待，他朝其车子挡风玻璃扔了一包冰毒后开车离开。当晚18时许，陈某，4开车在洪山桥头附近接上其和其朋友杨某，4，三人一起坐车到福州市晋安区鹤林新城对面公交站旁边等被告人林隆川，途中陈某，4将现金人民币300元给其，其通过微信转账人民币300元给被告人林隆川。陈某，4下车捡起烟盒交给其，其从烟盒拿出一袋塑料自封袋包装的白色晶体，确认是冰毒后交给陈某4，陈某，4拿到冰毒后开车带着其和杨某，4离开公交站，在车上陈某，4觉得一个冰毒不够玩让其也买一个，其找杨某，4凑足人民币300元通过微信转账给被告人林隆川，同时和被告人林隆川约定交易地点还是在福州市晋安区鹤林新城对面的公交站。杨某，4离开，随后民警将胡某，4、当晚21时许，被告人林隆川开车来到其车边上，将窗户放下扔出一个蓝色七匹狼烟盒，然后就开车走了。其回到家中玩了一个多小时，陈某，4转账人民币300元给其向其购买冰毒，其让他到鹤林新城某号楼楼下拿冰毒，其拿了一袋冰毒到一楼准备卖给陈某，4时被公安民警抓获。大约是当晚10点，其回到家中后胡某，4又联系其再买一袋冰毒，并通过微信转账给其人民币300元，约定交易地点不变。其将冰毒捡起来交给胡某，4，胡某，4朋友开车带着其和胡某，4离开，之后其、杨某，4各200元人民币辛苦费，杨某，4没有收，胡某，4收下后说一个冰毒太少不够玩，主动联系刚才贩卖毒品的人通过微信转账人民币300元再买一个冰毒，还让其把车开回鹤林新城对面的公交站。胡某，4将纸团内的一袋透明自封袋包装白色晶体放进自己上衣口袋内后，其开车带着胡某，4、其下车捡起烟盒并交给胡某，4，胡某，4打开烟盒从里面拿出一个用透明自封袋包装的白色晶体交给其，之后其开车带着胡某，4、在民警的安排下，当天下午18时许，其在福州市鼓楼区洪山桥头附近中石化加油站接到胡某，4以及胡某，4带来的小弟杨某，4，当时其负责开车，胡某，4坐在副驾驶位，杨某，4坐在后排，三人一起到福州市晋安区鹤林新城对面的公交站等贩卖毒品的人出现。杨某，4下车将纸团捡起交给其，其打开纸巾发现里面是一袋塑料自封袋包装的白色晶体便放进自己上衣口袋，陈某，4开车带其离开，之后警察将其抓获。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cut_words(sentence):\n",
    "    words = jieba.cut(sentence, cut_all=False)\n",
    "    tem = \" \".join(words).split()\n",
    "    return [i for i in tem if not i in stopwords]\n",
    "\n",
    "\n",
    "def rerank_sentences(query_words, candidate):\n",
    "    candidate_sentences = list(split_sentence(candidate))\n",
    "    corpus = [cut_words(s) for s in candidate_sentences]\n",
    "    bm25Model = bm25.BM25(corpus)\n",
    "    sentence_scores = np.array(bm25Model.get_scores(query_words))\n",
    "    filtered_candidate_sentences = [sent for sent, score in zip(\n",
    "        candidate_sentences, sentence_scores) if score > 0]\n",
    "    filtered_sentence_scores = sentence_scores[sentence_scores > 0]\n",
    "    reranked_sentences = [filtered_candidate_sentences[i]\n",
    "                          for i in filtered_sentence_scores.argsort().tolist()[::-1]]\n",
    "    uniqued_reranked_sentences = list(unique_sentences(reranked_sentences))\n",
    "    return ''.join(uniqued_reranked_sentences)\n",
    "\n",
    "\n",
    "rerank_sentences(cut_words(example_query), example_candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [07:11<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data_path, has_label=False):\n",
    "    query_path = osp.join(data_path, 'query.json')\n",
    "    all_candidates_path = osp.join(data_path, 'candidates')\n",
    "    if has_label:\n",
    "        label_path = osp.join(data_path, 'label_top30_dict.json')\n",
    "        label = json.load(open(label_path))\n",
    "        labels = []\n",
    "    edges, inputs, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs = [\n",
    "    ], [], [], [], [], []\n",
    "    with open(query_path) as f:\n",
    "        query_lines = f.readlines()\n",
    "    for query_line in tqdm(query_lines):\n",
    "        query_line = query_line.strip()\n",
    "        query_dict = json.loads(query_line)\n",
    "        input_str = \"。\".join(query_dict['crime']) + \\\n",
    "            \"。\"+query_dict['q']\n",
    "        query_words = cut_words(query_dict['q'])\n",
    "        tokenized_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "        query_idx = len(inputs)\n",
    "        inputs.append(tokenized_inputs)\n",
    "        node_graph_ids.append(len(query_ridxs))\n",
    "        query_ridxs.append(query_dict['ridx'])\n",
    "        query_ridx = str(query_dict['ridx'])\n",
    "        candidates_path = osp.join(all_candidates_path, query_ridx)\n",
    "        for candidate in os.listdir(candidates_path):\n",
    "            candidate_ridx = candidate[:-5]\n",
    "            candidate_path = osp.join(candidates_path, candidate)\n",
    "            candidate_dict = json.load(open(candidate_path))\n",
    "            all_text = ''.join(candidate_dict.values())\n",
    "            crime_name = crime_pattern.search(all_text)\n",
    "            if crime_name is None:\n",
    "                crime_name = ''\n",
    "            else:\n",
    "                crime_name = crime_name.group(1) + '罪'\n",
    "            candidate_text = rerank_sentences(\n",
    "                query_words, candidate_dict['ajjbqk'])\n",
    "            candidate_text = '。'.join(\n",
    "                [crime_name, candidate_text])\n",
    "            tokenized_candidate = tokenizer(\n",
    "                candidate_text, return_tensors=\"pt\")\n",
    "            candidate_idx = len(inputs)\n",
    "            inputs.append(tokenized_candidate)\n",
    "            node_graph_ids.append(node_graph_ids[-1])\n",
    "            edge_graph_ids.append(node_graph_ids[-1])\n",
    "            edges.append([query_idx, candidate_idx])\n",
    "            candidate_ridxs.append(int(candidate_ridx))\n",
    "            if has_label:\n",
    "                if candidate_ridx in label[query_ridx]:\n",
    "                    labels.append(label[query_ridx][candidate_ridx])\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "    if has_label:\n",
    "        return inputs, edges, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs, labels\n",
    "    return inputs, edges, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs\n",
    "\n",
    "\n",
    "train_path = '../data/origin/train'\n",
    "train_inputs, train_edges, train_query_ridxs, train_node_graph_ids, train_edge_graph_ids, train_candidate_ridxs, train_labels = preprocess_data(\n",
    "    train_path, has_label=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(processed_path, edges, inputs, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs, labels=None):\n",
    "    torch.save(inputs, osp.join(processed_path, 'inputs.pt'))\n",
    "    torch.save(edges, osp.join(processed_path, 'edges.pt'))\n",
    "    torch.save(query_ridxs, osp.join(processed_path, 'query_ridxs.pt'))\n",
    "    torch.save(candidate_ridxs, osp.join(processed_path, 'candidate_ridxs.pt'))\n",
    "    torch.save(node_graph_ids, osp.join(processed_path, 'node_graph_ids.pt'))\n",
    "    torch.save(edge_graph_ids, osp.join(processed_path, 'edge_graph_ids.pt'))\n",
    "    if labels is not None:\n",
    "        torch.save(labels, osp.join(processed_path, 'labels.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed_path = '../data/bm25/train/processed'\n",
    "os.makedirs(train_processed_path, exist_ok=True)\n",
    "save(train_processed_path, train_edges, train_inputs, train_query_ridxs,\n",
    "     train_node_graph_ids, train_edge_graph_ids, train_candidate_ridxs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = '../data/origin/test'\n",
    "test_inputs, test_edges, test_query_ridxs, test_node_graph_ids, test_edge_graph_ids, test_candidate_ridxs = preprocess_data(\n",
    "    test_path, has_label=False)\n",
    "test_processed_path = '../data/bm25/test/processed'\n",
    "os.makedirs(test_processed_path, exist_ok=True)\n",
    "save(test_processed_path, test_edges, test_inputs, test_query_ridxs,\n",
    "     test_node_graph_ids, test_edge_graph_ids, test_candidate_ridxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_query_ridxs = train_query_ridxs[:17]\n",
    "edge_split_idx = (np.array(train_edge_graph_ids) < 17).sum()\n",
    "node_split_idx = (np.array(train_node_graph_ids) < 17).sum()\n",
    "val_inputs = train_inputs[:node_split_idx]\n",
    "val_edges = train_edges[:edge_split_idx]\n",
    "val_labels = train_labels[:edge_split_idx]\n",
    "val_candidate_ridxs = train_candidate_ridxs[:edge_split_idx]\n",
    "val_node_graph_ids = train_node_graph_ids[:node_split_idx]\n",
    "val_edge_graph_ids = train_edge_graph_ids[:edge_split_idx]\n",
    "val_processed_path = '../data/bm25/val/processed'\n",
    "os.makedirs(val_processed_path, exist_ok=True)\n",
    "save(val_processed_path, val_edges, val_inputs, val_query_ridxs,\n",
    "     val_node_graph_ids, val_edge_graph_ids, val_candidate_ridxs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_query_ridxs = train_query_ridxs[17:]\n",
    "train_dev_inputs = train_inputs[node_split_idx:]\n",
    "train_dev_edges = np.array(train_edges[edge_split_idx:]) - node_split_idx\n",
    "train_dev_edges = train_dev_edges.tolist()\n",
    "train_dev_labels = train_labels[edge_split_idx:]\n",
    "train_dev_candidate_ridxs = train_candidate_ridxs[edge_split_idx:]\n",
    "train_dev_node_graph_ids = np.array(train_node_graph_ids[node_split_idx:]) - 17\n",
    "train_dev_edge_graph_ids = np.array(train_edge_graph_ids[edge_split_idx:]) - 17\n",
    "train_dev_node_graph_idx = train_dev_node_graph_ids.tolist()\n",
    "train_dev_edge_graph_idx = train_dev_edge_graph_ids.tolist()\n",
    "train_dev_processed_path = '../data/bm25/train_dev/processed'\n",
    "os.makedirs(train_dev_processed_path, exist_ok=True)\n",
    "save(train_dev_processed_path, train_dev_edges, train_dev_inputs,\n",
    "     train_dev_query_ridxs, train_dev_node_graph_ids, train_dev_edge_graph_ids,\n",
    "     train_dev_candidate_ridxs, train_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3db371a07bed52793c8840e411d9d35d61e1cfd36a2896481af3a875f3ddc4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('search')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
