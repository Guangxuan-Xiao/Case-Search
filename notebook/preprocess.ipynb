{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/chinese-roberta-wwm-ext\")\n",
    "crime_pattern = re.compile(r'已构成(.*?)罪')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_path, has_label=False):\n",
    "    query_path = osp.join(data_path, 'query.json')\n",
    "    all_candidates_path = osp.join(data_path, 'candidates')\n",
    "    if has_label:\n",
    "        label_path = osp.join(data_path, 'label_top30_dict.json')\n",
    "        label = json.load(open(label_path))\n",
    "        labels = []\n",
    "    edges, inputs, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs = [\n",
    "    ], [], [], [], [], []\n",
    "    with open(query_path) as f:\n",
    "        query_lines = f.readlines()\n",
    "    for query_line in tqdm(query_lines):\n",
    "        query_line = query_line.strip()\n",
    "        query_dict = json.loads(query_line)\n",
    "        input_str = \"[SEP]\".join(query_dict['crime']) + \\\n",
    "            \"[SEP]\"+query_dict['q']\n",
    "        tokenized_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "        query_idx = len(inputs)\n",
    "        inputs.append(tokenized_inputs)\n",
    "        node_graph_ids.append(len(query_ridxs))\n",
    "        query_ridxs.append(query_dict['ridx'])\n",
    "        query_ridx = str(query_dict['ridx'])\n",
    "        candidates_path = osp.join(all_candidates_path, query_ridx)\n",
    "        for candidate in os.listdir(candidates_path):\n",
    "            candidate_ridx = candidate[:-5]\n",
    "            candidate_path = osp.join(candidates_path, candidate)\n",
    "            candidate_dict = json.load(open(candidate_path))\n",
    "            all_text = ''.join(candidate_dict.values())\n",
    "            crime_name = crime_pattern.search(all_text)\n",
    "            if crime_name is None:\n",
    "                crime_name = ''\n",
    "            else:\n",
    "                crime_name = crime_name.group(1) + '罪'\n",
    "            candidate_text = '[SEP]'.join(\n",
    "                [crime_name, candidate_dict['ajjbqk']])\n",
    "            tokenized_candidate = tokenizer(\n",
    "                candidate_text, return_tensors=\"pt\")\n",
    "            candidate_idx = len(inputs)\n",
    "            inputs.append(tokenized_candidate)\n",
    "            node_graph_ids.append(node_graph_ids[-1])\n",
    "            edge_graph_ids.append(node_graph_ids[-1])\n",
    "            edges.append([query_idx, candidate_idx])\n",
    "            candidate_ridxs.append(int(candidate_ridx))\n",
    "            if has_label:\n",
    "                if candidate_ridx in label[query_ridx]:\n",
    "                    labels.append(label[query_ridx][candidate_ridx])\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "    if has_label:\n",
    "        return inputs, edges, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs, labels\n",
    "    return inputs, edges, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs\n",
    "train_path = '../data/train'\n",
    "train_inputs, train_edges, train_query_ridxs, train_node_graph_ids, train_edge_graph_ids, train_candidate_ridxs, train_labels = preprocess_data(\n",
    "    train_path, has_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(processed_path, edges, inputs, query_ridxs, node_graph_ids, edge_graph_ids, candidate_ridxs, labels=None):\n",
    "    torch.save(inputs, osp.join(processed_path, 'inputs.pt'))\n",
    "    torch.save(edges, osp.join(processed_path, 'edges.pt'))\n",
    "    torch.save(query_ridxs, osp.join(processed_path, 'query_ridxs.pt'))\n",
    "    torch.save(candidate_ridxs, osp.join(processed_path, 'candidate_ridxs.pt'))\n",
    "    torch.save(node_graph_ids, osp.join(processed_path, 'node_graph_ids.pt'))\n",
    "    torch.save(edge_graph_ids, osp.join(processed_path, 'edge_graph_ids.pt'))\n",
    "    if labels is not None:\n",
    "        torch.save(labels, osp.join(processed_path, 'labels.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/xgx/search-engine-project/notebook/preprocess.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Boctave/home/xgx/search-engine-project/notebook/preprocess.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m ic(\u001b[39mlen\u001b[39m(train_inputs), \u001b[39mlen\u001b[39m(train_edges), \u001b[39mlen\u001b[39m(train_query_ridxs), \u001b[39mlen\u001b[39m(train_node_graph_ids), \u001b[39mlen\u001b[39m(train_edge_graph_ids), \u001b[39mlen\u001b[39m(train_candidate_ridxs), \u001b[39mlen\u001b[39m(train_labels))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ic' is not defined"
     ]
    }
   ],
   "source": [
    "ic(len(train_inputs), len(train_edges), len(train_query_ridxs), len(train_node_graph_ids), len(train_edge_graph_ids), len(train_candidate_ridxs), len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed_path = '../data/train/processed'\n",
    "save(train_processed_path, train_edges, train_inputs, train_query_ridxs,\n",
    "     train_node_graph_ids, train_edge_graph_ids, train_candidate_ridxs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.16it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = '../data/test'\n",
    "test_inputs, test_edges, test_query_ridxs, test_node_graph_ids, test_edge_graph_ids, test_candidate_ridxs = preprocess_data(\n",
    "    test_path, has_label=False)\n",
    "test_processed_path = '../data/test/processed'\n",
    "save(test_processed_path, test_edges, test_inputs, test_query_ridxs,\n",
    "     test_node_graph_ids, test_edge_graph_ids, test_candidate_ridxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_query_ridxs = train_query_ridxs[:17]\n",
    "edge_split_idx = (np.array(train_edge_graph_ids) < 17).sum()\n",
    "node_split_idx = (np.array(train_node_graph_ids) < 17).sum()\n",
    "val_inputs = train_inputs[:node_split_idx]\n",
    "val_edges = train_edges[:edge_split_idx]\n",
    "val_labels = train_labels[:edge_split_idx]\n",
    "val_candidate_ridxs = train_candidate_ridxs[:edge_split_idx]\n",
    "val_node_graph_ids = train_node_graph_ids[:node_split_idx]\n",
    "val_edge_graph_ids = train_edge_graph_ids[:edge_split_idx]\n",
    "val_processed_path = '../data/val/processed'\n",
    "save(val_processed_path, val_edges, val_inputs, val_query_ridxs,\n",
    "     val_node_graph_ids, val_edge_graph_ids, val_candidate_ridxs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_query_ridxs = train_query_ridxs[17:]\n",
    "train_dev_inputs = train_inputs[node_split_idx:]\n",
    "train_dev_edges = np.array(train_edges[edge_split_idx:]) - node_split_idx\n",
    "train_dev_edges = train_dev_edges.tolist()\n",
    "train_dev_labels = train_labels[edge_split_idx:]\n",
    "train_dev_candidate_ridxs = train_candidate_ridxs[edge_split_idx:]\n",
    "train_dev_node_graph_ids = np.array(train_node_graph_ids[node_split_idx:]) - 17\n",
    "train_dev_edge_graph_ids = np.array(train_edge_graph_ids[edge_split_idx:]) - 17\n",
    "train_dev_node_graph_idx = train_dev_node_graph_ids.tolist()\n",
    "train_dev_edge_graph_idx = train_dev_edge_graph_ids.tolist()\n",
    "train_dev_processed_path = '../data/train_dev/processed'\n",
    "save(train_dev_processed_path, train_dev_edges, train_dev_inputs,\n",
    "     train_dev_query_ridxs, train_dev_node_graph_ids, train_dev_edge_graph_ids,\n",
    "     train_dev_candidate_ridxs, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3db371a07bed52793c8840e411d9d35d61e1cfd36a2896481af3a875f3ddc4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('search')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
